<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Agent with the Big Picture: Perceiving Surroundings for Interactive Instruction Following">
  <meta name="keywords" content="Embodied AI, Surrounding Perception, Modularization, Object-Centric">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OASIS: Online Sample Selection for Continual Visual Instruction Tuning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://98minjae.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <!-- <a class="navbar-item" href="https://dbd05088.github.io/projects/CL-ALFRED">
            CL-ALFRED
          </a> -->
          <a class="navbar-item" href="https://dbd05088.github.io/projects/EARL">
            EARL
          </a>
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered is-full-width">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size:42px">
          	OASIS: Online Sample Selection for Continual Visual Instruction Tuning
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bhkim94.github.io">Minjae Lee</a><sup>1</sup>*,
            </span>
            <span class="author-block">
              <a href="https://dbd05088.github.io">Minhyuk Seo</a><sup>2</sup>*,
            </span>
            <span class="author-block">
              Tingyu Qu<sup>2</sup>,
            </span>
            <span class="author-block">
              Tinne Tuytelaars<sup>2</sup>,
            </span>
            <span class="author-block">
              Jonghyun Choi<sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Seoul National University</span>,
            <span class="author-block"><sup>2</sup>KU Leuven</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.02011"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://github.com/yonseivnl/earl"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              <!-- Online continual learning suffers from an underfitted solution due to insufficient training for prompt model update (e.g., single-epoch training). 
              To address the challenge, we propose an efficient online continual learning method using the neural collapse phenomenon. 
              In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the continuously learned model with a single epoch can better fit to the streamed data by proposing <b>preparatory data training</b> and <b>residual correction</b> in the representation space, named <b>Equi-Angular Representation Learning (EARL)</b>. 
              With an extensive set of empirical validations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we show that our proposed method outperforms state-of- the-art methods by a noticeable margin in various online continual learning scenarios such as disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups. -->
              Despite the strong adaptation to user-specific preferences, recent Multimodal Large Language Models (MLLMs) face the risks of overfitting and delays in training time when training with increasing number of large-scale visual instruction tuning data in a continual learning (CL) setting. 
              To address the challenge, we propose Online Adaptive Sample selection via Informative Statistics (OASIS)
              , which (i) adaptively adjusts the number of selected samples per batch based on informativeness
              and (ii) reduces redundancy based on sample-wise similarity.
              With experiment results with various continual visual instruction tuning (CVIT) benchmarks, we show that OASIS outperforms the state-of-the-art in both LLaVA-1.5 and Qwen-VL-2.5 model architectures.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <div style="TEXT-ALIGN: center">
<iframe width="720" height="450" src="https://www.youtube.com/embed/wogdKWL596I?si=w_8yhHuYVLUvNngS" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div> -->

<section class="section">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">OASIS: Online Sample Selection for Continual Visual Instruction Tuning</h2>
    <div class="content has-text-justified">
      <p>
        OASIS first scores Informativeness for all samples in the online batch. 
        Then, through SIREN, OASIS approximates the change in Informativeness when assuming high informative samples in the same batch are trained.
        Finally, OASIS transforms Informativeness to Relative Informativeness (i.e., Informativeness of each sample compared to other batch samples)
        and performs Bernoulli sampling based on Relative Informativeness of each sample.
        <!-- Despite the success of the fixed ETF classifier in both the imbalanced training and offline CL, the ETF classifier has not yet been explored for online CL due to the necessity of sufficient training for neural collapse. 
        To be specific, streamed data are trained only once in online CL, which makes it harder to induce neural collapse than in offline CL which supports multi-epoch training.  -->
        
      </p>
      <p>
        <!-- To learn a better converged model without multi-epoch training for online CL, we propose two novel methods, each for the training phase and the inference phase, respectively. 
        In the training phase, we accelerate convergence by proposing preparatory data training. 
        In the inference phase, we propose to correct the remaining discrepancy between the classifiers and the features using residual correction.	 -->
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <img src='static/figures/overview.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
        </div>
      </div>
      <p></p>
      <h4 class="title is-3">What is SIREN?</h4>
        SIREN, short for Similarity-Aware Information Redundancy Elimination, prevents us from selecting similar samples in the same batch despite high Informativeness. 
        As similar task data tend to arrive together, samples in the same batch are highly likely to have similar information.
        Therefore, selecting fixed number of high informative/diverse samples in each batch like current online data selection do results in redundant training.
        To reduce redundancy, we adjust Informativeness of each sample based on samplewise similarity. 
        We iteratively identify the highest informative sample and reduce Informativness of the rest of the samples based on their similarity to the identified sample.
        <!-- For preparatory data to have a different representation from existing classes when trained, their image semantics should be distinguishable from the existing classes, provided that they contain enough semantic information to be considered as an image (i.e., not a noise). 
        There are various approaches to synthesize images with modified semantics, such as using the generative model or the negative transformation. 
        We use the negative transformation, as the generative model is expensive in computation and storage, which is undesirable for CL scenarios with limited memory and computations. 
        On the contrary, negative transformation is relatively efficient in computation and storage and is reported to create images with semantics different from the original image. 
        Speficially, we use negative rotation (rotation 90, 180, and 270 degrees) as negative transformation.
        We empirically observed that data generated through negative rotation have a distinct semantic class from the original class. -->
      <p></p>
        <h4 class="title is-3">What is Relative Informativenss?</h4>
        Informativeness of each sample doesn't quantify the relative importance of the sample compared to samples in the other batches.
        Therefore, we transform Informativeness to Relative Informativeness to estimate each sample’s relative position with respect to the average Informativeness of encountered samples.
        In doing so, we apply Z-score normalization to Informativeness to quantify a sample’s deviation from the overall Informativness distribution.
        As shown in the figure below, since observed data approximates a normal distribution, it is feasible to use Z-score normalization.
          <!-- For preparatory data to have a different representation from existing classes when trained, their image semantics should be distinguishable from the existing classes, provided that they contain enough semantic information to be considered as an image (i.e., not a noise). 
          There are various approaches to synthesize images with modified semantics, such as using the generative model or the negative transformation. 
          We use the negative transformation, as the generative model is expensive in computation and storage, which is undesirable for CL scenarios with limited memory and computations. 
          On the contrary, negative transformation is relatively efficient in computation and storage and is reported to create images with semantics different from the original image. 
          Speficially, we use negative rotation (rotation 90, 180, and 270 degrees) as negative transformation.
          We empirically observed that data generated through negative rotation have a distinct semantic class from the original class. -->
  
        <p></p>
        <div class="columns is-centered has-text-centered">
          <div class="column is-fullwidth">
            <img src='./static/figures/informativenss.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:80%;max-width:80%">
          </div>
        </div>
        <p></p>
        <p> 
          <!-- We summarize and illustrate the effect of each component in EARL.
          (a) In online CL, features of novel classes are biased towards the features of the previous class. 
          (b) By training with preparatory data, we address the bias problem. 
          (c) In inference, for features that do not fully converge to an ETF classifier, we add residuals to features that have not yet reached the corresponding classifier vectors, making features aligned with them. Purple arrow: the ‘residual correction’, Colors: classes. </p> -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-fullwidth">
            <img src='static/figures/Approach.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:80%;max-width:80%">
          </div>
        </div> -->
        <!-- <h4 class="subtitle has-text-centered">
          Effect of <span class="dnerf">Equi-Angular Representation Learning (EARL). </span>
        </h4> -->
    </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

    </div>
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">
        <p>
          <!-- We compare the accuracy of the online continual learning methods, including EARL. 
          As shown in the table, EARL outperforms other baselines on all benchmarks (CIFAR-10/100, TinyImageNet, ImageNet-200), both in disjoint and Gaussian-scheduled setups by large absolute margins. 
          In particular, high A_<sub>AUC</sub> (area under the accuracy curve) suggests that EARL outperforms other methods for all the time that the data stream is provided to the model, which implies that it can be used for inference at anytime. -->
          We compare the accuracy of the data selection methods, including OASIS.
          As shown in both tables, OASIS outperforms other baselines on all selection ratios in both LLaVA-1.5 and Qwen-VL-2.5 model architectures.
        </p>
        <!-- <p> -->
          <!-- For more details, please check out the <span style="color: #305fac; font-weight:bold"><a href="https://embodied-ai.org/papers/Agent-with-the-Big-Picture.pdf">paper</a></span>. -->
        <!-- </p> -->
        <br>
        <div class="columns is-centered has-text-centered">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
              <img src="static/tables/results_llava.png">
            </div>
          </div>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
              <img src="static/tables/results_llava.png">
              <h5>Comparison with State of the Art</h5>
            </div>
          </div>
        </div>

        <!-- <p>
          Furthermore, we measure the Average Online Accuracy (AOA), which uses the newly encountered streaming data for evaluation before incorporating it into the training process.
        </p>
        <div class="columns is-centered has-text-centered">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
              <img src="static/figures/AOA.png">
              <h5> AOA (Average Online Accuracy) Plot</h5>
            </div>
          </div> -->
        <!-- </div> -->

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
      <h2 class="title is-3">Computational cost of OASIS</h2>
      <div class="content has-text-justified">
        <p>
          We conduct an ablation study on the two components of OASIS, ORIN and SIREN. Although both components contribute to performance improvement, ORIN training shows a larger gain in performance.
        </p>
        <br>
        <div class="columns is-centered has-text-centered">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
              <img src="static/figures/acc_flops.png" style="width: 70%; height: auto;">
              <h5> Accuracy and FLOPs with 25% selection ratio on MICVIT. </h5>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
      <h2 class="title is-3">Ablation Study</h2>
      <div class="content has-text-justified">
        <p>
          We conduct an ablation study on the two components of OASIS, ORIN and SIREN. Although both components contribute to performance improvement, ORIN training shows a larger gain in performance.
        </p>
        <br>
        <div class="columns is-centered has-text-centered">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
              <img src="static/tables/ablation.png">
              <!-- <h5>Ablation Study. RC and PDT refer to preparatory data training and the residual correction.</h5> -->
            </div>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{kim2021agent,
  author    = {Kim, Byeonghwi and Bhambri, Suvaansh and Singh, Kunal Pratap and Mottaghi, Roozbeh and Choi, Jonghyun},
  title     = {Agent with the Big Picture: Perceiving Surroundings for Interactive Instruction Following},
  booktitle = {Embodied AI Workshop @ CVPR 2021},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The website template is from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Default Statcounter code for ABP
https://dbd05088.github.io/projects/EARL/ -->
<script type="text/javascript">
var sc_project=12971292;
var sc_invisible=1;
var sc_security="bf2d8e59";
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12971292/0/bf2d8e59/1/"
alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

</body>
</html>
